{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exhaustive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves to visualize the search space of the `NASbench` problem.\n",
    "It is a simple brute force search over all possible architectures.\n",
    "The search space is quite large, making it impossible to map each solution to its corresponding performance, due to memory constraints.\n",
    "What we can do however, is create a loop in which we still evaluate all possible architectures, and only store the number of times we encounter the performance value.\n",
    "This way we can still visualize the distribution of the performance values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nasbench.api import NASBench\n",
    "from main import init_ipynb, nas_ioh\n",
    "from utils import get_directories\n",
    "\n",
    "DIRS = get_directories(os.path.join(os.path.abspath(''), 'exhaustive_search.ipynb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'full'\n",
    "assert version in ['full', 'only108']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the NASbench object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = NASBench(f'../data/nasbench_{version}.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining dummy variables for interop with `main.py`.\n",
    "They do not have any real impact on what is being done here, as we never call the `main()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(optimizer='GA', population_size=100, mu_=40, lambda_=60, budget=5000, recombination='kp', selection='rw', mutation='uniform', xp=1, mut_r=None, mut_b=None, run_id=None, verbose=0, seed=42, repetitions=20, log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the NASbench object to `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ipynb(NB, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define possibilities for both parts of the bitstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_bitstrings = [b for b in product([0, 1], repeat=21) if sum(b) <= 9]\n",
    "print(f'Number of bitstring possibilities for matrix part: {len(mat_bitstrings)}')\n",
    "\n",
    "ops_bitstrings = [b for b in product([0, 1, 2], repeat=5)]\n",
    "print(f'Number of bitstring possibilities for operations part: {len(ops_bitstrings)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every matrix bitstring, we create a list of all possible final bitstrings and evaluate them.\n",
    "The total number of function calls to `nas_ioh()` will be `695,860 * 243 = 196,093,980`.\n",
    "This might take a while... (4 hours on MacBook Air M1 for 108, 6 hours for full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for i, mat_bs in enumerate(mat_bitstrings):\n",
    "    print(f'\\r{i/len(mat_bitstrings)*100:.2f}%', end='')\n",
    "    for bs in [mat_bs + ops_bs for ops_bs in ops_bitstrings]: scores[score] = 1 if not (score:=nas_ioh(bs)) in scores else scores[score] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort scores by score (descending)\n",
    "scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[0], reverse=True)}\n",
    "# convert to pandas dataframe\n",
    "scores_df = pd.DataFrame.from_dict(scores, orient='index', columns=['count'])\n",
    "# save to csv\n",
    "scores_df.to_csv(DIRS['csv'] + f'scores_dist_{version}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(DIRS['csv'] + f'scores_dist_{version}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the distribution of all of the performance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(scores_df.index.values, scores_df['count'].values, s=5, alpha=0.5)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('score')\n",
    "ax.set_ylabel('occurrences')\n",
    "ax.set_title(f'Distribution of NASbench scores ({version})')\n",
    "fig.tight_layout()\n",
    "fig.savefig(DIRS['plots'] + f'scores_dist_{version}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba309399f570c57919de3bfb36c195c6481f6cdea1861c2eb2b107e7cf184e6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
